# Day 3 â€” Building AI Agents with Memory
## ğŸ’¡ Goal: Understand how we can make a language model â€œremember,â€ â€œlearn,â€ and â€œcontinue conversationsâ€ across time â€” instead of treating every message like a brand-new chat.

### 1ï¸âƒ£ Stateless vs Stateful LLMs
#### Stateless LLM (How it normally works)

A normal language model, like when you use ChatGPT in API mode, forgets everything after each call.
Every message you send must contain all the information the model needs. Example:
```User: Whatâ€™s my name?
Model: I donâ€™t know unless you tell me.
```
If you say later:
```
User: Remind me of my name again?
```
It canâ€™t answer â€” because it doesnâ€™t remember you told it before.

â†’ This is stateless behavior â€” each request is independent.

#### Stateful LLM (What agents aim for)
A stateful system stores what happened before.
It can recall past facts, decisions, and context between turns or even between sessions. Example:
```
User: My name is Alex.
Model: Nice to meet you, Alex!
```
Later:
```User: Whatâ€™s my name?
Model: You told me earlier itâ€™s Alex.
```
Now it behaves like a human conversation â€” it â€œremembers.â€
To make this possible, we need context engineering, sessions, and memory.

### 2ï¸âƒ£ Context Engineering (Short-term attention span)

#### ğŸ“– Concept

An LLM can only â€œseeâ€ a limited number of tokens at once â€” its context window (like short-term memory).
Context engineering is the art of deciding what information goes into that window every time we talk to the model.

Instead of always sending the entire chat history, we send only whatâ€™s relevant.

#### âš ï¸ Problem: â€œContext rotâ€

If we keep stuffing in too much text, the model gets confused â€” important details drown in irrelevant ones.
We call this context rot.

#### How we fix it

- Summarization: Use the LLM itself to summarize old parts of the conversation.
- Pruning: Delete outdated or irrelevant details.
- Dynamic injection: Bring in only the pieces the current question needs.
- Example
  - Imagine a cooking assistant:
  - Yesterday you talked about â€œpasta recipes.â€
  - Today you ask: â€œRemind me of the sauce we made last time.â€
  - The system dynamically finds and injects only the â€œsauce recipeâ€ paragraph â€” not the whole conversation.

Thatâ€™s context engineering â€” keeping the signal strong and the noise low.

### 3ï¸âƒ£ Sessions ğŸ—‚ï¸

(Temporary workspace â€” like a tab in your browser)

A session groups together all the exchanges from one conversation.
It remembers messages, tool calls, and partial results while youâ€™re chatting.

When the session ends (you close the tab or finish the chat), its data may expire or be summarized for storage.

#### ğŸ§© Analogy

Think of a session like a whiteboard in a meeting room:
You write ideas on it while discussing, and erase or snapshot them at the end.
ğŸ§© Example
- Session 1: â€œPlan my trip to Tokyo.â€
- Session 2: â€œPlan my trip to Paris.â€
Each has its own short-term notes. They donâ€™t interfere, but both can later feed into long-term memory.

### 4ï¸âƒ£ Memory ğŸ§³

(Long-term knowledge â€” like a personal assistantâ€™s brain)

While sessions store temporary context, memory stores knowledge that persists across many sessions.

#### ğŸ“š Two types

| Type                   | Meaning                       | Example                                               |
| ---------------------- | ----------------------------- | ----------------------------------------------------- |
| **Declarative Memory** | Facts about the user or world | â€œSophia prefers Python over R.â€                       |
| **Procedural Memory**  | Learned processes or habits   | â€œWhen Sophia uploads a CSV, run data cleaning first.â€ |

#### âš™ï¸ How it works inside

- Vector Database: Stores text chunks as embeddings (numbers). Enables semantic search â€” â€œfind things similar to this question.â€
- Knowledge Graph: Stores explicit relationships (â€œAlex â†’ lives in â†’ Parisâ€).
Together they form a hybrid memory â€” flexible + structured.

#### Analogy
- Memory system: your personal assistantâ€™s notebook
- RAG (Retrieval-Augmented Generation): a librarian searching a bookshelf for references
- Agents often use both â€” memory for personal context, RAG for public knowledge.

#### 5ï¸âƒ£ Memory Lifecycle âš™ï¸

(How it learns and updates)

Building memory isnâ€™t just saving everything.
We need an automated process that extracts, cleans, and updates useful facts.

This is done via an LLM-driven ETL Pipeline:

| Step          | Meaning                            | Example                                    |
| ------------- | ---------------------------------- | ------------------------------------------ |
| **Extract**   | Find new facts from chats          | â€œUser said: My birthday is May 5.â€         |
| **Transform** | Normalize & check conflicts        | Remove duplicates; resolve contradictions. |
| **Load**      | Save into memory DB asynchronously | Update user_profile â†’ birthday = May 5.    |


Asynchronous means something happens in the background without waiting â€” it doesnâ€™t block or pause the main process.

ğŸ§© Example:
While your AI agent keeps chatting with you, another task (like saving memory or fetching data) runs quietly in the background and finishes later.

### 6ï¸âƒ£ Retrieving Memories ğŸ”

When the agent needs to answer a question, it searches its memory.
But what to retrieve depends on three scores:

| Factor                 | Meaning                                                |
| ---------------------- | ------------------------------------------------------ |
| **Semantic Relevance** | How similar the memory meaning is to the new question. |
| **Recency**            | How recently that memory was updated.                  |
| **Importance**         | How critical it is for user context.                   |

These are combined into a blended score.
The top memories are then injected into the next model prompt.

ğŸ§© Example

User: â€œBook a restaurant for my anniversary.â€

The system retrieves:
- â€œUserâ€™s partner = Jamie.â€
- â€œUser prefers Italian food.â€
- â€œAnniversary date = Nov 20.â€

Now the model can act with context, like a thoughtful assistant.

### 7ï¸âƒ£ Where Memories Are Inserted in the Prompt 

Where you place retrieved memory text in the prompt affects behavior.
| Placement                | Pros                           | Cons                          |
| ------------------------ | ------------------------------ | ----------------------------- |
| **System Instructions**  | Treated as truth; sets rules.  | Can cause bias if wrong.      |
| **Conversation History** | Feels natural to LLM dialogue. | Might confuse dialogue order. |

Designers often test both to see which yields better responses.

### 8ï¸âƒ£ Testing and Evaluation ğŸ§ª

To know whether memory helps:

- Precision / Recall: Does it fetch the right facts?
- Latency: How fast (< 200 ms)?
- Task Success: Does it actually help the user complete goals?

Only after this testing can we trust a memory system in production.

### 9ï¸âƒ£ The Big Picture 

| Layer                   | Purpose                                 | Analogy                   |
| ----------------------- | --------------------------------------- | ------------------------- |
| **Context Engineering** | Manage short-term info within a session | Working memory            |
| **Sessions**            | Keep conversation state active          | Meeting whiteboard        |
| **Memory**              | Store knowledge across sessions         | Long-term assistant brain |

These layers together turn a model from a text predictor into a learning partner.

#### Example Visualization â€” â€œHuman-Like Agent Lifecycleâ€
```
Day 1: "My favorite color is blue."
        â†“  (Extract â†’ Transform â†’ Load)
Memory updated: {favorite_color: "blue"}

Day 3: "Show me color palettes Iâ€™d like."
        â†“  (Retrieve)
Context injected: "User likes blue."
        â†“
Model generates palettes dominated by blue tones ğŸ¨
```
### ğŸ” Key Takeaways

- LLMs are naturally forgetful; context engineering simulates memory by managing what they â€œsee.â€
- Sessions organize short-term context for each conversation.
- Memory systems make agents truly personal and persistent.
- Hybrid storage + retrieval strategies (vector + graph + ETL) mirror human cognition.
- Evaluation ensures these memories actually improve user experience, not just complexity.
In Simple Words

- Context is what the model sees now.
- Session is what itâ€™s doing right now.
- Memory is what it will remember next time.
